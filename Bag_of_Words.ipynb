{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enguyen120/BigDataProject/blob/main/Bag_of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cam7e6lKV8XM"
      },
      "source": [
        "https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/03-TF-IDF-Scikit-Learn.html\n",
        "\n",
        "is where most of this came from\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mdjk5P1PYCR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible place for pre-processing: \n",
        "\n",
        "https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781787285217/7/ch07lvl1sec72/identifying-and-removing-rare-words\n",
        "\n",
        "lemmatization from here: \n",
        "https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/ \n",
        "\n"
      ],
      "metadata": {
        "id": "Y76hz6-sDJ1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W_BV7iquXN0",
        "outputId": "944a8be6-3ce3-4b8c-e608-677565e8c36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XB4AfnRzxQO"
      },
      "outputs": [],
      "source": [
        "# Do imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "# pd.set_option(\"max_rows\", 600)\n",
        "from pathlib import Path  \n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkyWx9v9PZpE",
        "outputId": "a5842178-c2fe-4b4b-9226-b57cdcfc86de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTq0USNwWLol",
        "outputId": "c769c4d4-fd60-46fb-dacb-4a3bd69cca11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr8quc0w1fus"
      },
      "outputs": [],
      "source": [
        "# Combine our data into one pandas df, called \"df\"\n",
        "# https://www.geeksforgeeks.org/how-to-merge-multiple-csv-files-into-a-single-pandas-dataframe/\n",
        "filenames = ['/content/drive/MyDrive/Big Data Final/articles1.csv',\n",
        "              '/content/drive/MyDrive/Big Data Final/articles2.csv',\n",
        "              '/content/drive/MyDrive/Big Data Final/articles3.csv']\n",
        "df = pd.concat(map(pd.read_csv, filenames), ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select articles that were published in the november of 2016 \n",
        "df = df[df.month == 11.0]\n",
        "df = df[df.year == 2016.0]"
      ],
      "metadata": {
        "id": "FHu18GG5TOZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save df as csv\n",
        "df.to_csv('/content/drive/MyDrive/Big Data Final/Novdf.csv')"
      ],
      "metadata": {
        "id": "5WlS2IVzZWph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc2x5o9tz3hp"
      },
      "outputs": [],
      "source": [
        "# Create a TFIDF Vectorizer that takes in strings and gives us a bag of words\n",
        "# ignores any word with freq < 10\n",
        "tfidf_vectorizer = TfidfVectorizer(input='content', stop_words='english', min_df = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90wuFXoo4uSv"
      },
      "outputs": [],
      "source": [
        "# Get a list of strings containing the content of each article (\"corpus\")\n",
        "# and a list containing the title of each article (\"titles\")\n",
        "corpus = [text for text in df['content']]\n",
        "titles = [title for title in df['title']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT PREPROCESSING HERE (or after \"fit\", idk where exactly)\n",
        "# https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781787285217/7/ch07lvl1sec72/identifying-and-removing-rare-words\n",
        "stoplist = stopwords.words('english')\n",
        "corpus2 = []\n",
        "for article in corpus:\n",
        "  # tokens = [word.lower() for word in nltk.regexp_tokenize(article, '\\w+')]\n",
        "  tokens = [word.lower() for word in nltk.regexp_tokenize(article, '[a-zA-Z]+')] \n",
        "  without_stops = [word for word in tokens if word not in stoplist]\n",
        "  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in without_stops]\n",
        "  cleaned_text = ' '.join(lemm_text)\n",
        "  corpus2.append(cleaned_text)"
      ],
      "metadata": {
        "id": "zvgmGmg9E0zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del corpus\n",
        "corpus = corpus2"
      ],
      "metadata": {
        "id": "mf4qBCluLL8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhbVmzF94avM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0930766-4872-469d-ca98-68ea8c96fe9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(min_df=10, stop_words='english')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Fit the vectorizer to the corpus - ie, the vectorizer now knows what words exist.\n",
        "tfidf_vectorizer.fit(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m7j-LMJFOhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1412bb-d673-46e1-84ac-1a908bd53b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# # This is what we'll be doing eventually:\n",
        "max = len(corpus)\n",
        "numlist = list(range(0, max, 10))\n",
        "\n",
        "# For now, using 1/10 of the corpus:\n",
        "# import math\n",
        "# max = int(math.floor(len(corpus) / 100))\n",
        "# numlist = list(range(0, max, 10))\n",
        "\n",
        "#Create the df for the result\n",
        "tfidf_df = pd.DataFrame()\n",
        "#For every interval of 10,\n",
        "for i in range(len(numlist)):\n",
        "  if(i < len(numlist) - 1):\n",
        "    # create a bag of words as a sparse matrix\n",
        "    sparsemat = tfidf_vectorizer.transform(corpus[numlist[i]:numlist[i+1]])\n",
        "    # get relevant titles\n",
        "    rownames = titles[numlist[i]:numlist[i+1]]\n",
        "    # turn BoW into a dense matrix, then into a df\n",
        "    test_df = pd.DataFrame(sparsemat.toarray(), index=rownames, columns=tfidf_vectorizer.get_feature_names())\n",
        "    # add it to the df for the result\n",
        "    tfidf_df = pd.concat([tfidf_df, test_df], axis = 0, ignore_index = True)\n",
        "    #NOTE: if we want the rows to have the names of the articles, get rid of \"ignore_index\" above\n",
        "  else:\n",
        "    tfidf_vectorizer.transform(corpus[i:max])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save df of the result\n",
        "tfidf_df.to_csv('/content/drive/MyDrive/Big Data Final/tfidf_df_pt1.csv')"
      ],
      "metadata": {
        "id": "Aq-O_7obGxFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cpqc6NM_8t5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e773174d-f23c-4e5b-88f1-138df2a52a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most common: ['trump', 'said', 'clinton', 'state', 'people', 'election', 'year', 'president', 'new', 'say'] \n",
            "\n",
            "\n",
            "10 least common: ['tolerable', 'compulsion', 'zeroed', 'herring', 'pettiness', 'vindictiveness', 'subsumed', 'unproductive', 'preface', 'perpetuates']\n"
          ]
        }
      ],
      "source": [
        "# Get top 10 most/least common words\n",
        "import numpy as np\n",
        "stupid_dict = {}\n",
        "for col in tfidf_df.columns:\n",
        "    stupid_dict[col] = np.sum(tfidf_df[col])\n",
        "stupid_list = [k for k, v in sorted(stupid_dict.items(), key=lambda item: item[1], reverse = True)]\n",
        "print(\"10 most common:\", stupid_list[0:10], \"\\n\\n\")\n",
        "print(\"10 least common:\", stupid_list[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "code scraps below ⬇️ --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "OFBngRAY9oA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf_vectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "80l2kKQgRteU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print df for the result\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "id": "_KXADgWKoywb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb5e1e95-6032-4c58-edfe-861d4ee8f3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      aaron  aaronkleinshow   ab  aback  abadi  abandon  abandoned  \\\n",
            "0       0.0             0.0  0.0    0.0    0.0      0.0   0.015958   \n",
            "1       0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "2       0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "3       0.0             0.0  0.0    0.0    0.0      0.0   0.019500   \n",
            "4       0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "...     ...             ...  ...    ...    ...      ...        ...   \n",
            "7975    0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "7976    0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "7977    0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "7978    0.0             0.0  0.0    0.0    0.0      0.0   0.056128   \n",
            "7979    0.0             0.0  0.0    0.0    0.0      0.0   0.000000   \n",
            "\n",
            "      abandoning  abandonment  abbas  ...  zimbabwe  zionist  zip  zombie  \\\n",
            "0            0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "1            0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "2            0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "3            0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "4            0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "...          ...          ...    ...  ...       ...      ...  ...     ...   \n",
            "7975         0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "7976         0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "7977         0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "7978         0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "7979         0.0          0.0    0.0  ...       0.0      0.0  0.0     0.0   \n",
            "\n",
            "      zone  zoo  zoom  zooming  zucker  zuckerberg  \n",
            "0      0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "1      0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "2      0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "3      0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "4      0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "...    ...  ...   ...      ...     ...         ...  \n",
            "7975   0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "7976   0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "7977   0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "7978   0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "7979   0.0  0.0   0.0      0.0     0.0         0.0  \n",
            "\n",
            "[7980 rows x 16725 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stoplist"
      ],
      "metadata": {
        "id": "Znfz62A_RbqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df)"
      ],
      "metadata": {
        "id": "jUgKhtQSRLUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# test_df = pd.DataFrame()\n",
        "# test_df_2 = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]})\n",
        "# test_df_3 = pd.concat([test_df, test_df_2], axis = 0, ignore_index = True)\n",
        "# print(test_df_3)\n",
        "# test_df_4 = pd.DataFrame(data={'col1': [8, 9], 'col2': [13, 0]})\n",
        "# test_df_3 = pd.concat([test_df_3, test_df_4], axis = 0, ignore_index = True)\n",
        "# print(test_df_3)"
      ],
      "metadata": {
        "id": "QXdxOlfUWyKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numlist = list(range(0, 132, 10))\n",
        "# for i in range(len(numlist)):\n",
        "#   if(i < len(numlist) - 1):\n",
        "#     print(numlist[i], numlist[i+1])\n",
        "#   else:\n",
        "#     print(numlist[i], 132)"
      ],
      "metadata": {
        "id": "7hvc0941UCQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRe-qPQZDPHj"
      },
      "outputs": [],
      "source": [
        "# densearray = tfidf_vector.toarray() #this is where the crash happens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M17GEhTb16UH"
      },
      "outputs": [],
      "source": [
        "# tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=titles, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "\n",
        "#TESTS TO MAKE IT SMALLER\n",
        "# for i in range(0, tfidf_vector.shape[0], 10):\n",
        "#   tfidf_df = pd.DataFrame(tfidf_vector[i - 10, i].toarray(), index=titles, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "# while(i > 0):\n",
        "#   if(i > 10):\n",
        "#     tfidf_df = pd.DataFrame(tfidf_vector[i - 10, i].toarray(), index=titles, columns=tfidf_vectorizer.get_feature_names())\n",
        "#     i -= 10\n",
        "#   else:\n",
        "#     tfidf_df = pd.DataFrame(tfidf_vector[0, i].toarray(), index=titles, columns=tfidf_vectorizer.get_feature_names())\n",
        "#     i = 0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}